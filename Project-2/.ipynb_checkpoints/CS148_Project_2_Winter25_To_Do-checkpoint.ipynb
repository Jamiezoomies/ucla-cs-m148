{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-m80KgQ1-Z3"
   },
   "source": [
    "## 25W-COM SCI-M148 Project 2 - Binary Classification Comparative Methods\n",
    "\n",
    "Name:\n",
    "\n",
    "UID:\n",
    "\n",
    "### **Submission Guidelines**\n",
    "1. Please fill in your name and UID above.\n",
    "\n",
    "2. Please submit a **PDF printout** of your Jupyter Notebook to **Gradescope**. If you have any trouble accessing Gradescope, please let a TA know ASAP.  \n",
    "\n",
    "3. As the PDF can get long, please tag the respective sections to ensure the readers know where to look.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSR9laCN1-Z5"
   },
   "source": [
    "For this project we're going to attempt a binary classification of a dataset using multiple methods and compare results.\n",
    "\n",
    "Our goals for this project will be to introduce you to several of the most common classification techniques, how to perform them and tweek parameters to optimize outcomes, how to produce and interpret results, and compare performance. You will be asked to analyze your findings and provide explanations for observed performance.\n",
    "\n",
    "Specifically you will be asked to classify whether a <b>patient is suffering from heart disease</b> based on a host of potential medical factors.\n",
    "\n",
    "<b><u>DEFINITIONS</b></u>\n",
    "\n",
    "\n",
    "<b> Binary Classification:</b>\n",
    "In this case a complex dataset has an added 'target' label with one of two options. Your learning algorithm will try to assign one of these labels to the data.\n",
    "\n",
    "<b> Supervised Learning:</b>\n",
    "This data is fully supervised, which means it's been fully labeled and we can trust the veracity of the labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zacxg4dy1-Z7"
   },
   "source": [
    "## Background: The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKL6UhzY1-Z8"
   },
   "source": [
    "For this exercise, we will be using a subset of the <b>UCI Heart Disease dataset</b>. This dataset was created by collecting clinical data from patients undergoing diagnostic tests for heart disease. All identifying information about the patients has been removed to protect their privacy. The dataset represents data from patients who were suspected of having heart disease and underwent several diagnostic tests, including blood tests, electrocardiograms (ECG), exercise stress tests, and fluoroscopic imaging. \n",
    "\n",
    "The dataset includes 14 columns. The information provided by each column is as follows:\n",
    "<ul>\n",
    "    <li><b>age:</b> Patient age in years</li>\n",
    "    <li><b>sex:</b> Patient sex (1 = male; 0 = female)</li>\n",
    "    <li><b>c_pain:</b> Chest pain type (0 = asymptomatic; 1 = atypical angina (unusual discomfort due to reduced blood flow to the heart); 2 = non-anginal pain (chest pain unrelated to the heart); 3 = typical angina (classic chest discomfort due to reduced blood flow to the heart))</li>\n",
    "    <li><b>rbp:</b> Resting blood pressure in mm Hg (measured at hospital admission)</li>\n",
    "    <li><b>chol:</b> Serum cholesterol level in mg/dL</li>\n",
    "    <li><b>high_fbs:</b> Fasting blood sugar > 120 mg/dL (1 = true; 0 = false)</li>\n",
    "    <li><b>r_ecg:</b> Resting electrocardiographic results (0 = probable thickened left ventricular wall; 1 = normal; 2 = ST-T wave abnormality)</li>\n",
    "    <li><b>hr_max:</b> Maximum heart rate achieved during the stress test</li>\n",
    "    <li><b>has_ex_ang:</b> Exercise-induced angina (1 = yes; 0 = no)</li>\n",
    "    <li><b>ecg_depress:</b> Depression of the ST segment on ECG during exercise compared to rest (measured in mm)</li>\n",
    "    <li><b>stress_slope:</b> Slope of the peak exercise ST segment (0 = downsloping (concerning); 1 = flat (abnormal); 2 = upsloping (normal))</li>\n",
    "    <li><b>num_vessels:</b> Number of major vessels (0–3) showing good blood flow during fluoroscopy</li>\n",
    "    <li><b>thal_test_res:</b> Thallium Stress Test result (assesses blood flow using trace amounts of radioactive thallium-201) (1 = normal; 2 = fixed defect; 7 = reversible defect)</li>\n",
    "    <li><b>heart_disease:</b> Indicates whether heart disease is present (True = Disease; False = No disease)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-OS5fRR1-Z9"
   },
   "source": [
    "## Loading Essentials and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89pf3hlo1-Z-"
   },
   "outputs": [],
   "source": [
    "#Here are a set of libraries we imported to complete this assignment.\n",
    "#Feel free to use these or equivalent libraries for your implementation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
    "import os\n",
    "import seaborn as sns # used for plot interactive graph.\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics.cluster as smc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLA8MesY1-aD"
   },
   "source": [
    "## Part 1. Load the Data and Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cPvNFAb1-aD"
   },
   "source": [
    "Let's first load our dataset so we'll be able to work with it. (correct the relative path if your notebook is in a different directory than the csv file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeQ4T7nD1-aE"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('heartdisease.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR8RcYxs1-aF"
   },
   "source": [
    "### Now that our data is loaded, let's take a closer look at the dataset we're working with. Use the head method,  the describe method, and the info method to display some of the rows so we can visualize the types of data fields we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPdUGLr21-aF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muZ1U7Xw1-aG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TS41dFMJYskm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hl4xIdL1-aM"
   },
   "source": [
    "### Before we begin our analysis we need to fix the field(s) that will be problematic. Specifically convert our boolean heart_disease variable into a binary numeric target variable (values of either '0' or '1'), and then drop the original heart_disease datafield from the dataframe. (hint: try label encoder or .astype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDmfWcDO1-aM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDbsCOog1-aN"
   },
   "source": [
    "### Now that we have a feel for the data-types for each of the variables, plot histograms of each field and attempt to ascertain how each variable performs (is it a binary, or limited selection, or does it follow a gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYlbbySv1-aO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZwpu5xZ1-aP"
   },
   "source": [
    "### We also want to make sure we are dealing with a balanced dataset. In this case, we want to confirm whether or not we have an equitable number of  sick and healthy individuals to ensure that our classifier will have a sufficiently balanced dataset to adequately classify the two. Plot a histogram specifically of the heart_disease target, and conduct a count of the number of diseased and healthy individuals and report on the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo0tH8sS1-aP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v1_1HFC1-aS"
   },
   "source": [
    "### Now that we have our dataframe prepared let's start analyzing our data. For this next question let's look at the correlations of our variables to our target value. First, map out the correlations between the values, and then discuss the relationships you observe. Do some research on the variables to understand why they may relate to the observed corellations. Intuitively, why do you think some variables correlate more highly than others (hint: one possible approach you can use the sns heatmap function to map the corr() method)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXAfCjem1-aS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osMk-hMm1-aT"
   },
   "source": [
    "[Discuss correlations here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc_52Cut1-aT"
   },
   "source": [
    "## Part 2. Prepare the 'Raw' Data and run a KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ6wr3Xc1-aU"
   },
   "source": [
    "Before running our various learning methods, we need to do some additional prep to finalize our data. Specifically you'll have to cut the classification target from the data that will be used to classify, and then you'll have to divide the dataset into training and testing cohorts.\n",
    "\n",
    "Specifically, we're going to ask you to prepare 2 batches of data: 1. Will simply be the raw numeric data that hasn't gone through any additional pre-processing. The other, will be data that you pipeline using your own selected methods. We will then feed both of these datasets into a classifier to showcase just how important this step can be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piz-mp0o1-aV"
   },
   "source": [
    "### Save the label column as a separate array and then drop it from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8svzGB-A1-aV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG7ri8V11-aW"
   },
   "source": [
    "### First Create your 'Raw' unprocessed training data by dividing your dataframe into training and testing cohorts, with your training cohort consisting of 70% of your total dataframe (hint: use the train_test_split() method) Output the resulting shapes of your training and testing samples to confirm that your split was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmjpma2F1-aX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N5-dXElZKE9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7TWR8en1-aa"
   },
   "source": [
    "### We'll explore how not processing your data can impact model performance by using the K-Nearest Neighbor classifier. One thing to note was because KNN's rely on Euclidean distance, they are highly sensitive to the relative magnitude of different features. Let's see that in action! Implement a K-Nearest Neighbor algorithm on our raw data and report the results. For this initial implementation simply use the **default** settings. Refer to the [KNN Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for details on implementation. Report on the accuracy of the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D60t7Qe31-ab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeGfA2Cf1-ab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofBgJMdH1-ac"
   },
   "source": [
    "### Now implement a pipeline of your choice. You can opt to handle categoricals however you wish, however please scale your numeric features using standard scaler. Use the fit_transform() to fit this pipeline to your training data. and then transform() to apply that pipeline to your test data\n",
    "\n",
    "Hint:\n",
    "1. Create separate pipelines for numeric and categorical features with Pipeline() and then combining them with ColumnTransformer()\n",
    "2. First, fit the full pipeline with the training data. Then, apply it to the test data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xySOY-yS1-ad"
   },
   "source": [
    "### Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgX7OEo61-af"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mde5kr1Z9fS"
   },
   "outputs": [],
   "source": [
    "# Create pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHQZzYiTJ3US"
   },
   "outputs": [],
   "source": [
    "# Pipeline the training and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMbUOszrJOM0"
   },
   "source": [
    "### Now retrain your model and compare the accuracy metrics (Accuracy, Precision, Recall, F1 Score) with the raw and pipelined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4JqymGE1-an"
   },
   "outputs": [],
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCRmyi1Y1-an"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Report Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjDfPNK01-ao"
   },
   "source": [
    "[Discuss Results here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3Kx4nH91-ap"
   },
   "source": [
    "### Parameter Optimization.  The KNN Algorithm includes an n_neighbors attribute that specifies how many neighbors to use when developing the cluster. (The default value is 5, which is what your previous model used.) Lets now try n values of: 1, 2, 3, 5, 7, 9, 10, 20, and 50. Run your model for each value and report the accuracy for each. (HINT leverage python's ability to loop to run through the array and generate results without needing to manually code each iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aQa6hMT1-aq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uq4htLuI1-aq"
   },
   "source": [
    "## Part 3. Additional Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2pXCSFS1-ar"
   },
   "source": [
    "So we have a model that seems to work well. But let's see if we can do better! To do so we'll employ multiple learning methods and compare result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qs89tu81-ar"
   },
   "source": [
    "### Linear Decision Boundary Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaZ_NkZM1-as"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtWdZgsY1-as"
   },
   "source": [
    "Let's now try another classifier,one that's well known for handling linear models: Logistic Regression. Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycUesx_E1-at"
   },
   "source": [
    "### Implement a Logistical Regression Classifier. Review the [Logistical Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for how to implement the model.\n",
    "\n",
    "### Report metrics for:\n",
    "1.   Accuracy\n",
    "2.   Precision\n",
    "3.   Recall\n",
    "4.   F1 Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZtnRX5f1-au"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxc0eyNm1-a7"
   },
   "source": [
    "### Discuss what each measure is reporting, why they are different, and why are each of these measures is significant. Explore why we might choose to evaluate the performance of differing models differently based on these factors. Try to give some specific examples of scenarios in which you might value one of these measures over the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu8lKGYL1-a7"
   },
   "source": [
    "[Provide explanation for each measure here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVc5lMGZ1-av"
   },
   "source": [
    "### Let's tweak a few settings. First let's set your solver to 'sag' (Stochastic Average Gradient), your max_iter= 10, and set penalty = None and rerun your model. Let's see how your results change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82CwH2261-aw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaLDf1Wn1-ax"
   },
   "source": [
    "### Did you notice that when you ran the previous model you got the following warning: \"ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\". Check the documentation and see if you can implement a fix for this problem, and again report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGaSV3wo1-ay"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1YFl1Y1-a0"
   },
   "source": [
    "### Explain what you changed, and why do you think that may have altered the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYiZLct01-a1"
   },
   "source": [
    "[Provide explanation here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M59dRwK81-a2"
   },
   "source": [
    "### Rerun your logistic classifier, but modify the penalty = 'l1', solver='liblinear' and again report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xsw3IKrs1-a2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUxx3eFy1-a3"
   },
   "source": [
    "### Explain what the two solver approaches are, and why liblinear may have produced an improved outcome (but not always, and it's ok if your results show otherwise!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJZIVyvS1-a4"
   },
   "source": [
    "[Provide explanation here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C95mrPjA1-a4"
   },
   "source": [
    "### SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJYSnWKf1-a5"
   },
   "source": [
    "A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUAfw9mF1-a5"
   },
   "source": [
    "### Implement a Support Vector Machine classifier on your pipelined data. Review the [SVM Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for how to implement a model. For this implementation you can simply use the default settings, but set probability = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kT3sHrW1-a6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVNvSFJt1-a6"
   },
   "source": [
    "### Report the accuracy, precision, recall, F1 Score, of your model, but in addition, plot a Confusion Matrix of your model's performance\n",
    "\n",
    "recommend using `from sklearn.metrics import ConfusionMatrixDisplay` for this one!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3If-zbZG1-a6"
   },
   "outputs": [],
   "source": [
    "# Report Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaXoiMYd1-a8"
   },
   "source": [
    "### Plot a Receiver Operating Characteristic curve, or ROC curve, and describe what it is and what the results indicate\n",
    "\n",
    "recommend using the `metrics.roc_curve` `metrics.auc` and `metrics.RocCurveDisplay` for this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7NfLZoL1-a9"
   },
   "source": [
    "[Describe what an ROC Curve is and what the results mean here] The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, so the areas under ROC curves are used to compare the usefulness of tests. Here we see a relatively low area under the curve indicating a poorly performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx7L-bgW1-a9"
   },
   "source": [
    "### Rerun your SVM, but now modify your model parameter kernel to equal 'linear'. Again report your Accuracy, Precision, Recall, F1 scores, and Confusion matrix and plot the new ROC curve.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6KpYps71-a-"
   },
   "outputs": [],
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wlnzo0uL1-a-"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgTiftKK1-a_"
   },
   "outputs": [],
   "source": [
    "# ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW_lqTMn1-bA"
   },
   "source": [
    "### Explain the what the new results you've achieved mean. Read the documentation to understand what you've changed about your model and explain why changing that input parameter might impact the results in the manner you've observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gdji_tZn1-bA"
   },
   "source": [
    "[Provide Answer here:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uoAdwas1-bB"
   },
   "source": [
    "### Both logistic regression and linear SVM are trying to classify data points using a linear decision boundary, then what’s the difference between their ways to find this boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZvy7RTF1-bC"
   },
   "source": [
    "[Provide Answer here:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Create both a Decision Tree and a KNN and fit them onto your fully preprocessed data, then calculate an accuracy score for both (https://scikit-learn.org/stable/api/sklearn.tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Decision Tree \n",
    "\n",
    "\n",
    "# KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Accuracy \n",
    "\n",
    "\n",
    "# KNN Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Preprocessing Only\n",
    "\n",
    "Create a new preprocessing pipeline which ONLY preprocesses categorical values (leaving scalar variables in the data as they were originally, ie. no StandardScaler).   \n",
    "Process your data with this new pipeline, fit a decision tree and a KNN once more and report a new accuracy score for each.   \n",
    "\n",
    "Hint: Ensure that remainder = 'passthrough' in your ColumnTransformer to ensure scalar values are not dropped!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Preprocessing Only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Decision Tree \n",
    "\n",
    "\n",
    "# Fit KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Accuracy \n",
    "\n",
    "\n",
    "# KNN Accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the difference in accuracy loss in Decision Trees vs KNNs when Standardization was removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwZkVmKRcKZY"
   },
   "source": [
    "# Printing Jupyter notebook to PDF (Google Colab Only, Optional)\n",
    "\n",
    "It may take a few minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUXRTQ6RcHAG"
   },
   "outputs": [],
   "source": [
    "def colab2pdf():\n",
    "  ENABLE=True # @param {type:\"boolean\"}\n",
    "  if ENABLE:\n",
    "    !apt-get install librsvg2-bin\n",
    "    import os, datetime, json, locale, pathlib, urllib, requests, werkzeug, nbformat, google, yaml, warnings\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "    NAME = pathlib.Path(werkzeug.utils.secure_filename(urllib.parse.unquote(requests.get(f\"http://{os.environ['COLAB_JUPYTER_IP']}:{os.environ['KMP_TARGET_PORT']}/api/sessions\").json()[0][\"name\"])))\n",
    "    TEMP = pathlib.Path(\"/content/pdfs\") / f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{NAME.stem}\"; TEMP.mkdir(parents=True, exist_ok=True)\n",
    "    NB = [cell for cell in nbformat.reads(json.dumps(google.colab._message.blocking_request(\"get_ipynb\", timeout_sec=600)[\"ipynb\"]), as_version=4).cells if \"--Colab2PDF\" not in cell.source]\n",
    "    warnings.filterwarnings('ignore', category=nbformat.validator.MissingIDFieldWarning)\n",
    "    with (TEMP / f\"{NAME.stem}.ipynb\").open(\"w\", encoding=\"utf-8\") as nb_copy: nbformat.write(nbformat.v4.new_notebook(cells=NB or [nbformat.v4.new_code_cell(\"#\")]), nb_copy)\n",
    "  if not pathlib.Path(\"/usr/local/bin/quarto\").exists():\n",
    "    !wget -q \"https://quarto.org/download/latest/quarto-linux-amd64.deb\" -P {TEMP} && dpkg -i {TEMP}/quarto-linux-amd64.deb > /dev/null && quarto install tinytex --update-path --quiet\n",
    "    with (TEMP / \"config.yml\").open(\"w\", encoding=\"utf-8\") as file: yaml.dump({'include-in-header': [{\"text\": r\"\\usepackage{fvextra}\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines,breakanywhere,commandchars=\\\\\\{\\}}\"}],'include-before-body': [{\"text\": r\"\\DefineVerbatimEnvironment{verbatim}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines}\"}]}, file)\n",
    "    !quarto render {TEMP}/{NAME.stem}.ipynb --metadata-file={TEMP}/config.yml --to pdf -M latex-auto-install -M margin-top=1in -M margin-bottom=1in -M margin-left=1in -M margin-right=1in --quiet\n",
    "    google.colab.files.download(str(TEMP / f\"{NAME.stem}.pdf\"))\n",
    "colab2pdf()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
